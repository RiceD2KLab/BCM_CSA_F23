{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models/cheap_features/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models: Finding cheap features to predict Central Sleep Apnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_loc = '../data/processed/cheap_features/'\n",
    "datasets = os.listdir('../data/processed/cheap_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ahi_c0h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "model_name = None\n",
    "best_mae = 100000\n",
    "best_dataset = None\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "    # Load data\n",
    "    df = pd.read_csv(folder_loc + dataset)\n",
    "\n",
    "    features = df.columns.tolist()\n",
    "    features.remove(target)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)\n",
    "\n",
    "    # Train xgboost\n",
    "\n",
    "    mae_xgb, model_xgb = train_model(XGBRegressor(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train random forest\n",
    "\n",
    "    mae_rf, model_rf = train_model(RandomForestRegressor(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train linear regression\n",
    "\n",
    "    mae_lr, model_lr = train_model(LinearRegression(), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train lasso\n",
    "\n",
    "    mae_lasso, model_lasso = train_model(Lasso(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train ridge\n",
    "\n",
    "    mae_ridge, model_ridge = train_model(Ridge(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train decision tree\n",
    "\n",
    "    mae_dt, model_dt = train_model(DecisionTreeRegressor(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Save best model\n",
    "\n",
    "    if mae_xgb < best_mae:\n",
    "        best_mae = mae_xgb\n",
    "        best_model = model_xgb\n",
    "        model_name = 'xgb'\n",
    "        best_dataset = dataset\n",
    "    if mae_rf < best_mae:\n",
    "        best_mae = mae_rf\n",
    "        best_model = model_rf\n",
    "        model_name = 'rf'\n",
    "        best_dataset = dataset\n",
    "    if mae_lr < best_mae:\n",
    "        best_mae = mae_lr\n",
    "        best_model = model_lr\n",
    "        model_name = 'lr'\n",
    "        best_dataset = dataset\n",
    "    if mae_lasso < best_mae:\n",
    "        best_mae = mae_lasso\n",
    "        best_model = model_lasso\n",
    "        model_name = 'lasso'\n",
    "        best_dataset = dataset\n",
    "    if mae_ridge < best_mae:\n",
    "        best_mae = mae_ridge\n",
    "        best_model = model_ridge\n",
    "        model_name = 'ridge'\n",
    "        best_dataset = dataset\n",
    "    if mae_dt < best_mae:\n",
    "        best_mae = mae_dt\n",
    "        best_model = model_dt\n",
    "        model_name = 'dt'\n",
    "        best_dataset = dataset\n",
    "\n",
    "# Save model\n",
    "import pickle\n",
    "pickle.dump(best_model, open('../models/cheap_features/' + model_name + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name, best_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = pd.read_csv('../data/interim/shhs-data-dictionary-0.20.0-variables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows where id == best_model.feature_names_in_\n",
    "features_dict = features_dict[features_dict['id'].isin(best_model.feature_names_in_)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict[['display_name', 'description', 'folder']].to_csv('./results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antropometric features and Lifestyle and Behavioural features are the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparemeter tuning of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and model\n",
    "import pickle\n",
    "\n",
    "best_model = pickle.load(open('../models/cheap_features/ridge.pkl', 'rb'))\n",
    "df = pd.read_csv('../data/processed/cheap_features/Ant_Lif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 4.434528937795567, 'solver': 'sparse_cg'}\n",
      "-4.723208689897129\n",
      "Ridge(alpha=4.434528937795567, random_state=1, solver='sparse_cg')\n"
     ]
    }
   ],
   "source": [
    "# do RandomizedSearchCV to find best hyperparameters\n",
    "model = Ridge(random_state=1)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': uniform(0, 10),\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_grid, n_iter=20, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, random_state=1)\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "print(search.best_estimator_)\n",
    "best_param = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.625547601873998\n",
      "4.56984217173576\n"
     ]
    }
   ],
   "source": [
    "# test on validation set\n",
    "model = Ridge(**best_param, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(mae)\n",
    "print(mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.75856854 -4.82324379 -4.75379298 -4.76452213 -4.51591534]\n",
      "-4.723208554189368\n"
     ]
    }
   ],
   "source": [
    "# cross validation training\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = Ridge(alpha=best_param['alpha'], solver=best_param['solver'], random_state=1)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(scores)\n",
    "print(scores.mean())\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('../data/interim/feature_selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dataset for dataset in datasets if 'ahi_c0h4a' in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_loc = '../data/interim/feature_selection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ahi_c0h4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:12<00:00, 38.60s/it]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "model_name = None\n",
    "best_mae = 100000\n",
    "best_dataset = None\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "    # Load data\n",
    "    df = pd.read_csv(folder_loc + dataset)\n",
    "\n",
    "    features = df.columns.tolist()\n",
    "    features.remove(target)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)\n",
    "\n",
    "    # Train xgboost\n",
    "\n",
    "    mae_xgb, model_xgb = train_model(XGBRegressor(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train random forest\n",
    "\n",
    "    mae_rf, model_rf = train_model(RandomForestRegressor(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train linear regression\n",
    "\n",
    "    mae_lr, model_lr = train_model(LinearRegression(), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train lasso\n",
    "\n",
    "    mae_lasso, model_lasso = train_model(Lasso(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train ridge\n",
    "\n",
    "    mae_ridge, model_ridge = train_model(Ridge(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Train decision tree\n",
    "\n",
    "    mae_dt, model_dt = train_model(DecisionTreeRegressor(random_state=1), X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "\n",
    "    # Save best model\n",
    "\n",
    "    if mae_xgb < best_mae:\n",
    "        best_mae = mae_xgb\n",
    "        best_model = model_xgb\n",
    "        model_name = 'xgb'\n",
    "        best_dataset = dataset\n",
    "    if mae_rf < best_mae:\n",
    "        best_mae = mae_rf\n",
    "        best_model = model_rf\n",
    "        model_name = 'rf'\n",
    "        best_dataset = dataset\n",
    "    if mae_lr < best_mae:\n",
    "        best_mae = mae_lr\n",
    "        best_model = model_lr\n",
    "        model_name = 'lr'\n",
    "        best_dataset = dataset\n",
    "    if mae_lasso < best_mae:\n",
    "        best_mae = mae_lasso\n",
    "        best_model = model_lasso\n",
    "        model_name = 'lasso'\n",
    "        best_dataset = dataset\n",
    "    if mae_ridge < best_mae:\n",
    "        best_mae = mae_ridge\n",
    "        best_model = model_ridge\n",
    "        model_name = 'ridge'\n",
    "        best_dataset = dataset\n",
    "    if mae_dt < best_mae:\n",
    "        best_mae = mae_dt\n",
    "        best_model = model_dt\n",
    "        model_name = 'dt'\n",
    "        best_dataset = dataset\n",
    "\n",
    "# Save model\n",
    "import pickle\n",
    "pickle.dump(best_model, open('../models/cheap_features/' + model_name + '_feature_selection.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree_ahi_c0h4a.csv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../data/interim/feature_selection/{best_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso 5.819445065906095\n"
     ]
    }
   ],
   "source": [
    "print(model_name, best_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best feature selection method is Decision Tree but the results are not as good as our simple feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparemeter tuning of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and model\n",
    "import pickle\n",
    "\n",
    "best_model = pickle.load(open('../models/cheap_features/lasso_feature_selection.pkl', 'rb'))\n",
    "df = pd.read_csv('../data/processed/cheap_features/Ant_Lif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 4.434528937795567, 'solver': 'sparse_cg'}\n",
      "-4.723208689897129\n",
      "Ridge(alpha=4.434528937795567, random_state=1, solver='sparse_cg')\n"
     ]
    }
   ],
   "source": [
    "# do RandomizedSearchCV to find best hyperparameters\n",
    "model = Ridge(random_state=1)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test, X_val, y_val = split_data(df, features, target)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': uniform(0, 10),\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_grid, n_iter=20, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, random_state=1)\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "print(search.best_estimator_)\n",
    "best_param = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.625547601873998\n",
      "4.56984217173576\n"
     ]
    }
   ],
   "source": [
    "# test on validation set\n",
    "model = Ridge(**best_param, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(mae)\n",
    "print(mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.75856854 -4.82324379 -4.75379298 -4.76452213 -4.51591534]\n",
      "-4.723208554189368\n"
     ]
    }
   ],
   "source": [
    "# cross validation training\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = Ridge(alpha=best_param['alpha'], solver=best_param['solver'], random_state=1)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(scores)\n",
    "print(scores.mean())\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
